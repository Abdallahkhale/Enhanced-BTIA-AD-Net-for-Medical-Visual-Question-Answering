# config/config.yaml
# Enhanced BTIA-AD Net Configuration

model:
  # Pretrained Models
  vision_encoder: "microsoft/BiomedCLIP-PubMedBERT_256-vit_base_patch16_224"
  text_encoder: "microsoft/BiomedNLP-BiomedBERT-base-uncased-abstract-fulltext"
  
  # Architecture
  type: "generative"
  text_decoder: "microsoft/BioGPT"
  freeze_vision: true  # Freeze vision encoder to save memory/time initially
  freeze_text: false   # Train decoder
  
  # Old Fusion (Ignored for Generative)
  fusion_method: "btia"
  use_answer_distillation: false
  use_embedding_matching: false
  
  # Two-Stage Training (LLaVA approach - CRITICAL for generative models)
  # Stage 1: Freeze LLM, train only projection layer (cross-modal alignment)
  # Stage 2: Unfreeze LLM, train projection + LLM (fine-tuning)
  pretraining_epochs: 10  # Number of epochs to train projection only (Stage 1) - longer alignment
  
  # LoRA Fine-tuning (prevents overfitting, 10x fewer params)
  use_lora: true
  lora_r: 16
  lora_alpha: 32

training:
  # Batch and Learning
  batch_size: 16
  learning_rate: 1.0e-5  # Lower LR for better LLM fine-tuning
  weight_decay: 0.01
  epochs: 50
  warmup_ratio: 0.1
  
  # RTX 5070 Ti Optimizations
  fp16: true
  gradient_accumulation_steps: 2
  gradient_checkpointing: true
  
  # Checkpointing
  save_every_epoch: true
  checkpoint_dir: "checkpoints"
  resume_from: null  # Start from scratch with new prompts
  
  # Early Stopping
  early_stopping_patience: 10
  early_stopping_metric: "val_accuracy"

data:
  # Image Settings
  image_size: 224
  
  # Text Settings
  max_question_length: 64
  max_answer_length: 32
  
  # Dataset Augmentation
  use_slake_augmentation: false  # SLAKE images not downloaded
  use_pathvqa_augmentation: true  # Add 32K+ pathology samples!
  train_split: 0.8
  num_workers: 4
  
  # Data Directories
  vqa_rad_dir: "data/vqa_rad"
  slake_dir: "data/slake"
  pathvqa_dir: "data/pathvqa"

logging:
  log_dir: "logs"
  log_every_n_steps: 10
  use_tensorboard: true
